{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c278f3e",
   "metadata": {},
   "source": [
    "# üéì AI-Powered Interactive Learning Assistant for Classrooms\n",
    "\n",
    "## OpenVINO Unnati Hackathon Project\n",
    "\n",
    "This notebook demonstrates the development of an intelligent classroom companion that enhances learning experiences through AI-powered features including question answering, content summarization, multimodal interaction, and personalized learning support.\n",
    "\n",
    "### üéØ Project Objectives\n",
    "\n",
    "- **Enhance Classroom Engagement**: Interactive Q&A and real-time feedback\n",
    "- **Improve Accessibility**: Multimodal support (text, speech, visual)\n",
    "- **Automate Content Generation**: Lesson summaries and explanations\n",
    "- **Personalize Learning**: Adaptive content based on student progress\n",
    "- **Optimize Performance**: OpenVINO acceleration for real-time interaction\n",
    "\n",
    "### üõ†Ô∏è Technology Stack\n",
    "\n",
    "- **AI Models**: LLMs, Transformers, Whisper, BLIP for multimodal capabilities\n",
    "- **Optimization**: OpenVINO for Intel CPU/GPU/NPU acceleration\n",
    "- **Backend**: FastAPI for robust API services\n",
    "- **Frontend**: Streamlit for interactive web interface\n",
    "- **Speech**: Whisper for STT, Coqui TTS for text-to-speech\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b89f9",
   "metadata": {},
   "source": [
    "## 1. Define the Use Case\n",
    "\n",
    "### üéØ Identifying Classroom Challenges\n",
    "\n",
    "Our AI-powered learning assistant addresses several key educational challenges:\n",
    "\n",
    "1. **Student Engagement**: Many students struggle to stay engaged during lectures\n",
    "2. **Accessibility**: Students with different learning styles need varied content formats\n",
    "3. **Real-time Support**: Teachers need instant help with content generation and Q&A\n",
    "4. **Personalization**: One-size-fits-all approach doesn't work for diverse learners\n",
    "5. **Content Summarization**: Students need quick summaries of complex lessons\n",
    "\n",
    "### üë• Target Users\n",
    "\n",
    "- **Primary**: Students (K-12 and higher education)\n",
    "- **Secondary**: Teachers and educators\n",
    "- **Tertiary**: Educational administrators\n",
    "\n",
    "### üìä Success Metrics\n",
    "\n",
    "- Response time < 2 seconds for real-time interaction\n",
    "- 95%+ accuracy in question answering\n",
    "- Multimodal support (text, speech, visual)\n",
    "- Seamless integration with existing educational workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79527d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and set up environment\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Data handling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set up project paths\n",
    "PROJECT_ROOT = Path(\"../\")  # Assuming notebook is in notebooks/ directory\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Environment setup completed!\")\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT.absolute()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "\n",
    "# Display system information\n",
    "import platform\n",
    "print(f\"üíª System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üèóÔ∏è  Architecture: {platform.architecture()[0]}\")\n",
    "\n",
    "# Check if we're in the correct environment\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PyTorch not installed\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Transformers not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a4b97",
   "metadata": {},
   "source": [
    "## 2. Select and Download AI Models\n",
    "\n",
    "### ü§ñ Model Selection Strategy\n",
    "\n",
    "We'll use a combination of open-source models optimized for educational tasks:\n",
    "\n",
    "| Task | Model | Source | Rationale |\n",
    "|------|--------|---------|-----------|\n",
    "| **Question Answering** | DistilBERT-QA | Hugging Face | Lightweight, fast, educational-friendly |\n",
    "| **Summarization** | BART-CNN | Hugging Face | Excellent for educational content |\n",
    "| **Speech Recognition** | Whisper-base | OpenAI | Multilingual, robust |\n",
    "| **Text-to-Speech** | Tacotron2 | Coqui TTS | Natural sounding voice |\n",
    "| **Image Captioning** | BLIP | Salesforce | Multimodal understanding |\n",
    "\n",
    "### üì¶ Model Download and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ca6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model download and initialization\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering, \n",
    "    AutoModelForSeq2SeqLM, pipeline\n",
    ")\n",
    "\n",
    "# Define model configurations\n",
    "MODELS_CONFIG = {\n",
    "    \"question_answering\": {\n",
    "        \"model_name\": \"distilbert-base-cased-distilled-squad\",\n",
    "        \"task\": \"question-answering\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"model_name\": \"facebook/bart-large-cnn\",\n",
    "        \"task\": \"summarization\"\n",
    "    },\n",
    "    \"text_generation\": {\n",
    "        \"model_name\": \"microsoft/DialoGPT-medium\",\n",
    "        \"task\": \"text-generation\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_and_cache_models():\n",
    "    \"\"\"Download and cache all required models\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    print(\"üì• Downloading AI models...\")\n",
    "    \n",
    "    for task, config in MODELS_CONFIG.items():\n",
    "        try:\n",
    "            print(f\"  üîÑ Loading {task} model: {config['model_name']}\")\n",
    "            \n",
    "            # Create pipeline for the task\n",
    "            models[task] = pipeline(\n",
    "                config[\"task\"],\n",
    "                model=config[\"model_name\"],\n",
    "                tokenizer=config[\"model_name\"],\n",
    "                device=-1  # Use CPU by default\n",
    "            )\n",
    "            \n",
    "            print(f\"  ‚úÖ {task} model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to load {task} model: {str(e)}\")\n",
    "            models[task] = None\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Download models (this may take a few minutes on first run)\n",
    "ai_models = download_and_cache_models()\n",
    "\n",
    "# Display model status\n",
    "print(\"\\nüìä Model Status Summary:\")\n",
    "for task, model in ai_models.items():\n",
    "    status = \"‚úÖ Ready\" if model is not None else \"‚ùå Failed\"\n",
    "    print(f\"  {task}: {status}\")\n",
    "\n",
    "print(f\"\\nüéØ Successfully loaded {sum(1 for m in ai_models.values() if m is not None)}/{len(ai_models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd8898",
   "metadata": {},
   "source": [
    "## 3. Install and Set Up OpenVINO\n",
    "\n",
    "### üöÄ OpenVINO Installation\n",
    "\n",
    "OpenVINO is Intel's toolkit for optimizing and deploying AI models on Intel hardware. It provides significant performance improvements for inference tasks.\n",
    "\n",
    "#### Key Benefits:\n",
    "- **Performance**: Up to 10x faster inference on Intel CPUs\n",
    "- **Efficiency**: Reduced memory usage and power consumption  \n",
    "- **Compatibility**: Supports multiple frameworks (PyTorch, TensorFlow, ONNX)\n",
    "- **Hardware Optimization**: Optimized for Intel CPUs, GPUs, and NPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f734e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and set up OpenVINO\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_openvino():\n",
    "    \"\"\"Install OpenVINO and related packages\"\"\"\n",
    "    packages = [\n",
    "        \"openvino==2024.5.0\",\n",
    "        \"optimum[openvino]\",\n",
    "        \"nncf\"  # Neural Network Compression Framework\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            print(f\"üì¶ Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "# Install OpenVINO (uncomment if not already installed)\n",
    "# install_openvino()\n",
    "\n",
    "# Import OpenVINO modules\n",
    "try:\n",
    "    import openvino as ov\n",
    "    from openvino.tools import mo\n",
    "    from optimum.intel import OVModelForQuestionAnswering, OVModelForSeq2SeqLM\n",
    "    \n",
    "    print(\"‚úÖ OpenVINO imported successfully!\")\n",
    "    print(f\"üîß OpenVINO version: {ov.__version__}\")\n",
    "    \n",
    "    # Initialize OpenVINO Core\n",
    "    core = ov.Core()\n",
    "    \n",
    "    # Check available devices\n",
    "    available_devices = core.available_devices\n",
    "    print(f\"üíª Available devices: {available_devices}\")\n",
    "    \n",
    "    # Display device capabilities\n",
    "    for device in available_devices:\n",
    "        try:\n",
    "            device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "            print(f\"  üì± {device}: {device_name}\")\n",
    "        except:\n",
    "            print(f\"  üì± {device}: Device information not available\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå OpenVINO import failed: {e}\")\n",
    "    print(\"üí° Please install OpenVINO: pip install openvino optimum[openvino]\")\n",
    "    core = None\n",
    "\n",
    "# Create models directory for OpenVINO IR files\n",
    "models_dir = Path(\"../models/openvino\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ OpenVINO models directory: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65fc51",
   "metadata": {},
   "source": [
    "## 4. Convert and Optimize Models with OpenVINO\n",
    "\n",
    "### üîÑ Model Conversion Process\n",
    "\n",
    "OpenVINO converts models to Intermediate Representation (IR) format for optimal performance:\n",
    "\n",
    "1. **Model Export**: Convert from original framework (PyTorch, TensorFlow)\n",
    "2. **Optimization**: Apply graph optimizations and quantization\n",
    "3. **Compilation**: Compile for specific hardware (CPU, GPU, NPU)\n",
    "\n",
    "### ‚ö° Performance Optimizations\n",
    "\n",
    "- **Graph Optimization**: Removes redundant operations\n",
    "- **Quantization**: Reduces precision (FP32 ‚Üí FP16 ‚Üí INT8)\n",
    "- **Batching**: Optimizes for batch processing\n",
    "- **Memory Layout**: Optimizes data layout for target hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and optimize models with OpenVINO\n",
    "def convert_model_to_openvino(model_name: str, task: str, device: str = \"CPU\"):\n",
    "    \"\"\"Convert a HuggingFace model to OpenVINO format\"\"\"\n",
    "    \n",
    "    if core is None:\n",
    "        print(\"‚ùå OpenVINO not available\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Converting {model_name} for {task}...\")\n",
    "        \n",
    "        # Choose appropriate OpenVINO model class\n",
    "        if task == \"question-answering\":\n",
    "            ov_model = OVModelForQuestionAnswering.from_pretrained(\n",
    "                model_name,\n",
    "                export=True,\n",
    "                device=device\n",
    "            )\n",
    "        elif task == \"summarization\":\n",
    "            ov_model = OVModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name,\n",
    "                export=True,\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Task {task} not supported for OpenVINO conversion yet\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úÖ Successfully converted {model_name}\")\n",
    "        return ov_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Conversion failed for {model_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Convert models to OpenVINO format\n",
    "openvino_models = {}\n",
    "\n",
    "if core is not None:\n",
    "    print(\"üöÄ Converting models to OpenVINO format...\")\n",
    "    \n",
    "    # Convert Question Answering model\n",
    "    qa_model_name = MODELS_CONFIG[\"question_answering\"][\"model_name\"]\n",
    "    openvino_models[\"qa\"] = convert_model_to_openvino(\n",
    "        qa_model_name, \n",
    "        \"question-answering\"\n",
    "    )\n",
    "    \n",
    "    # Convert Summarization model\n",
    "    sum_model_name = MODELS_CONFIG[\"summarization\"][\"model_name\"]\n",
    "    openvino_models[\"summarization\"] = convert_model_to_openvino(\n",
    "        sum_model_name,\n",
    "        \"summarization\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä OpenVINO Conversion Summary:\")\n",
    "    for task, model in openvino_models.items():\n",
    "        status = \"‚úÖ Converted\" if model is not None else \"‚ùå Failed\"\n",
    "        print(f\"  {task}: {status}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping OpenVINO conversion - OpenVINO not available\")\n",
    "\n",
    "# Example: Manual model conversion using OpenVINO Model Optimizer\n",
    "def demonstrate_manual_conversion():\n",
    "    \"\"\"Demonstrate manual model conversion process\"\"\"\n",
    "    \n",
    "    if core is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüîß Manual Conversion Example:\")\n",
    "    print(\"This is how you would convert a PyTorch model manually:\")\n",
    "    \n",
    "    example_code = '''\n",
    "    # Step 1: Export PyTorch model to ONNX\n",
    "    import torch\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\"distilbert-base-cased\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "        \"model.onnx\",\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"last_hidden_state\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "            \"last_hidden_state\": {0: \"batch_size\", 1: \"sequence\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Step 2: Convert ONNX to OpenVINO IR\n",
    "    from openvino.tools import mo\n",
    "    \n",
    "    ov_model = mo.convert_model(\"model.onnx\")\n",
    "    ov.save_model(ov_model, \"model.xml\")\n",
    "    \n",
    "    # Step 3: Load and compile OpenVINO model\n",
    "    core = ov.Core()\n",
    "    model = core.read_model(\"model.xml\")\n",
    "    compiled_model = core.compile_model(model, \"CPU\")\n",
    "    '''\n",
    "    \n",
    "    print(example_code)\n",
    "\n",
    "demonstrate_manual_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f6bc1",
   "metadata": {},
   "source": [
    "## 5. Benchmark Model Performance\n",
    "\n",
    "### üìä Performance Evaluation\n",
    "\n",
    "We'll compare the performance of original models vs OpenVINO optimized versions across:\n",
    "\n",
    "- **Inference Time**: Response latency for real-time interaction\n",
    "- **Throughput**: Requests per second capability  \n",
    "- **Memory Usage**: RAM and GPU memory consumption\n",
    "- **Accuracy**: Ensuring optimization doesn't hurt model quality\n",
    "\n",
    "### üéØ Classroom Performance Requirements\n",
    "\n",
    "For effective classroom use, we target:\n",
    "- **Question Answering**: < 2 seconds response time\n",
    "- **Summarization**: < 3 seconds for 1000 words\n",
    "- **Speech Recognition**: < 1 second processing\n",
    "- **Text-to-Speech**: < 2 seconds generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2d23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark model performance\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from typing import Dict, List, Callable\n",
    "\n",
    "def benchmark_model(model, test_input, model_name: str, num_runs: int = 10) -> Dict:\n",
    "    \"\"\"Benchmark a model's performance\"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        return {\"error\": \"Model not available\"}\n",
    "    \n",
    "    times = []\n",
    "    memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Warm up\n",
    "    try:\n",
    "        _ = model(test_input)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Run benchmarks\n",
    "    for i in range(num_runs):\n",
    "        gc.collect()  # Clean up memory\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = model(test_input)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in {model_name} run {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    if not times:\n",
    "        return {\"error\": \"All benchmark runs failed\"}\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"num_runs\": len(times),\n",
    "        \"mean_time\": np.mean(times),\n",
    "        \"std_time\": np.std(times),\n",
    "        \"min_time\": np.min(times),\n",
    "        \"max_time\": np.max(times),\n",
    "        \"median_time\": np.median(times),\n",
    "        \"memory_delta\": memory_after - memory_before\n",
    "    }\n",
    "\n",
    "# Define test inputs for benchmarking\n",
    "test_inputs = {\n",
    "    \"question_answering\": {\n",
    "        \"question\": \"What is artificial intelligence?\",\n",
    "        \"context\": \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of intelligent agents: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\"\n",
    "    },\n",
    "    \"summarization\": {\n",
    "        \"text\": \"\"\"\n",
    "        Machine learning is a method of data analysis that automates analytical model building. \n",
    "        It is a branch of artificial intelligence based on the idea that systems can learn from data, \n",
    "        identify patterns and make decisions with minimal human intervention. Machine learning algorithms \n",
    "        build mathematical models based on training data in order to make predictions or decisions without \n",
    "        being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of \n",
    "        applications, such as in medicine, email filtering, speech recognition, and computer vision, \n",
    "        where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\n",
    "        \"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = {}\n",
    "\n",
    "print(\"üîÑ Running performance benchmarks...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Benchmark original models\n",
    "if ai_models:\n",
    "    for task, model in ai_models.items():\n",
    "        if model is not None and task in test_inputs:\n",
    "            print(f\"\\nüìä Benchmarking original {task} model...\")\n",
    "            \n",
    "            test_input = test_inputs[task]\n",
    "            result = benchmark_model(model, test_input, f\"Original {task}\")\n",
    "            benchmark_results[f\"original_{task}\"] = result\n",
    "            \n",
    "            if \"error\" not in result:\n",
    "                print(f\"  ‚è±Ô∏è  Mean time: {result['mean_time']:.3f}s ¬± {result['std_time']:.3f}s\")\n",
    "                print(f\"  üöÄ Best time: {result['min_time']:.3f}s\")\n",
    "                print(f\"  üíæ Memory delta: {result['memory_delta']:.1f} MB\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {result['error']}\")\n",
    "\n",
    "# Benchmark OpenVINO models\n",
    "if openvino_models:\n",
    "    for task, model in openvino_models.items():\n",
    "        if model is not None:\n",
    "            print(f\"\\nüìä Benchmarking OpenVINO {task} model...\")\n",
    "            \n",
    "            # Map task names to test inputs\n",
    "            test_key = \"question_answering\" if task == \"qa\" else task\n",
    "            if test_key in test_inputs:\n",
    "                test_input = test_inputs[test_key]\n",
    "                result = benchmark_model(model, test_input, f\"OpenVINO {task}\")\n",
    "                benchmark_results[f\"openvino_{task}\"] = result\n",
    "                \n",
    "                if \"error\" not in result:\n",
    "                    print(f\"  ‚è±Ô∏è  Mean time: {result['mean_time']:.3f}s ¬± {result['std_time']:.3f}s\")\n",
    "                    print(f\"  üöÄ Best time: {result['min_time']:.3f}s\")\n",
    "                    print(f\"  üíæ Memory delta: {result['memory_delta']:.1f} MB\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå {result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Benchmarking completed!\")\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_data = []\n",
    "for key, result in benchmark_results.items():\n",
    "    if \"error\" not in result:\n",
    "        performance_data.append({\n",
    "            \"Model\": result[\"model_name\"],\n",
    "            \"Mean Time (s)\": result[\"mean_time\"],\n",
    "            \"Min Time (s)\": result[\"min_time\"],\n",
    "            \"Max Time (s)\": result[\"max_time\"],\n",
    "            \"Std Dev (s)\": result[\"std_time\"],\n",
    "            \"Memory Delta (MB)\": result[\"memory_delta\"]\n",
    "        })\n",
    "\n",
    "if performance_data:\n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    print(\"\\nüìà Performance Summary:\")\n",
    "    print(performance_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "if performance_data:\n",
    "    # Create performance comparison plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Mean inference time comparison\n",
    "    models = performance_df[\"Model\"]\n",
    "    mean_times = performance_df[\"Mean Time (s)\"]\n",
    "    \n",
    "    ax1.bar(models, mean_times, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "    ax1.set_title(\"Mean Inference Time Comparison\")\n",
    "    ax1.set_ylabel(\"Time (seconds)\")\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add performance threshold line\n",
    "    ax1.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='Target: 2s')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Min vs Max time comparison\n",
    "    min_times = performance_df[\"Min Time (s)\"]\n",
    "    max_times = performance_df[\"Max Time (s)\"]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, min_times, width, label='Min Time', color='lightblue')\n",
    "    ax2.bar(x + width/2, max_times, width, label='Max Time', color='lightcoral')\n",
    "    ax2.set_title(\"Min vs Max Inference Time\")\n",
    "    ax2.set_ylabel(\"Time (seconds)\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models, rotation=45)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    memory_delta = performance_df[\"Memory Delta (MB)\"]\n",
    "    \n",
    "    ax3.bar(models, memory_delta, color=['purple', 'green', 'orange', 'red'])\n",
    "    ax3.set_title(\"Memory Usage Delta\")\n",
    "    ax3.set_ylabel(\"Memory (MB)\")\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Standard deviation (consistency)\n",
    "    std_devs = performance_df[\"Std Dev (s)\"]\n",
    "    \n",
    "    ax4.bar(models, std_devs, color=['gold', 'silver', 'bronze', 'copper'])\n",
    "    ax4.set_title(\"Performance Consistency (Lower is Better)\")\n",
    "    ax4.set_ylabel(\"Standard Deviation (s)\")\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create interactive Plotly chart\n",
    "    fig_plotly = go.Figure()\n",
    "    \n",
    "    # Add bars for each model\n",
    "    for i, model in enumerate(models):\n",
    "        fig_plotly.add_trace(go.Bar(\n",
    "            name=model,\n",
    "            x=[model],\n",
    "            y=[mean_times.iloc[i]],\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=[std_devs.iloc[i]],\n",
    "                visible=True\n",
    "            ),\n",
    "            text=f\"{mean_times.iloc[i]:.3f}s\",\n",
    "            textposition=\"auto\"\n",
    "        ))\n",
    "    \n",
    "    # Add target line\n",
    "    fig_plotly.add_hline(\n",
    "        y=2.0, \n",
    "        line_dash=\"dash\", \n",
    "        line_color=\"red\",\n",
    "        annotation_text=\"Target: 2s\"\n",
    "    )\n",
    "    \n",
    "    fig_plotly.update_layout(\n",
    "        title=\"AI Model Performance Benchmark\",\n",
    "        xaxis_title=\"Model\",\n",
    "        yaxis_title=\"Inference Time (seconds)\",\n",
    "        showlegend=False,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig_plotly.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No performance data available for visualization\")\n",
    "\n",
    "# Calculate speedup if both original and OpenVINO results exist\n",
    "speedup_analysis = {}\n",
    "for task in [\"qa\", \"summarization\"]:\n",
    "    original_key = f\"original_question_answering\" if task == \"qa\" else f\"original_{task}\"\n",
    "    openvino_key = f\"openvino_{task}\"\n",
    "    \n",
    "    if (original_key in benchmark_results and openvino_key in benchmark_results and\n",
    "        \"error\" not in benchmark_results[original_key] and \"error\" not in benchmark_results[openvino_key]):\n",
    "        \n",
    "        original_time = benchmark_results[original_key][\"mean_time\"]\n",
    "        openvino_time = benchmark_results[openvino_key][\"mean_time\"]\n",
    "        speedup = original_time / openvino_time\n",
    "        \n",
    "        speedup_analysis[task] = {\n",
    "            \"original_time\": original_time,\n",
    "            \"openvino_time\": openvino_time,\n",
    "            \"speedup\": speedup,\n",
    "            \"improvement\": (original_time - openvino_time) / original_time * 100\n",
    "        }\n",
    "\n",
    "if speedup_analysis:\n",
    "    print(\"\\nüöÄ OpenVINO Speedup Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    for task, analysis in speedup_analysis.items():\n",
    "        print(f\"\\n{task.upper()}:\")\n",
    "        print(f\"  Original: {analysis['original_time']:.3f}s\")\n",
    "        print(f\"  OpenVINO: {analysis['openvino_time']:.3f}s\")\n",
    "        print(f\"  Speedup: {analysis['speedup']:.2f}x\")\n",
    "        print(f\"  Improvement: {analysis['improvement']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12f61d",
   "metadata": {},
   "source": [
    "## 6. Build the Interactive Assistant Interface\n",
    "\n",
    "### üé® User Interface Design\n",
    "\n",
    "Our learning assistant interface supports multiple interaction modes:\n",
    "\n",
    "1. **Text-based Q&A**: Traditional chat interface for questions\n",
    "2. **Document Upload**: PDF/text file processing and summarization  \n",
    "3. **Voice Interaction**: Speech-to-text and text-to-speech\n",
    "4. **Multimodal**: Combined text, voice, and visual inputs\n",
    "\n",
    "### üõ†Ô∏è Technology Choices\n",
    "\n",
    "- **Gradio**: For rapid prototyping and demos\n",
    "- **Streamlit**: For full-featured web applications\n",
    "- **FastAPI**: For robust backend API services\n",
    "- **WebRTC**: For real-time audio/video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d42f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build interactive interface with Gradio\n",
    "try:\n",
    "    import gradio as gr\n",
    "    gradio_available = True\n",
    "    print(\"‚úÖ Gradio is available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing Gradio...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gradio\"])\n",
    "    import gradio as gr\n",
    "    gradio_available = True\n",
    "\n",
    "# Define interface functions\n",
    "def process_question(question: str, context: str = \"\") -> str:\n",
    "    \"\"\"Process a question using the QA model\"\"\"\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    # Use OpenVINO model if available, otherwise fall back to original\n",
    "    model = openvino_models.get(\"qa\") or ai_models.get(\"question_answering\")\n",
    "    \n",
    "    if model is None:\n",
    "        return \"‚ùå Question answering model not available.\"\n",
    "    \n",
    "    try:\n",
    "        if context.strip():\n",
    "            full_context = context\n",
    "        else:\n",
    "            # Provide default educational context\n",
    "            full_context = \"This is an educational question in a classroom setting.\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if hasattr(model, 'predict'):  # OpenVINO model\n",
    "            result = model(question=question, context=full_context)\n",
    "        else:  # HuggingFace pipeline\n",
    "            result = model(question=question, context=full_context)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        answer = result.get('answer', str(result))\n",
    "        confidence = result.get('score', 0.0)\n",
    "        \n",
    "        response = f\"**Answer:** {answer}\\n\\n\"\n",
    "        response += f\"**Confidence:** {confidence:.2f}\\n\"\n",
    "        response += f\"**Processing Time:** {processing_time:.3f} seconds\"\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error processing question: {str(e)}\"\n",
    "\n",
    "def summarize_text(text: str, max_length: int = 150) -> str:\n",
    "    \"\"\"Summarize text using the summarization model\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"Please enter text to summarize.\"\n",
    "    \n",
    "    if len(text) < 50:\n",
    "        return \"Text is too short to summarize effectively.\"\n",
    "    \n",
    "    # Use OpenVINO model if available, otherwise fall back to original\n",
    "    model = openvino_models.get(\"summarization\") or ai_models.get(\"summarization\")\n",
    "    \n",
    "    if model is None:\n",
    "        return \"‚ùå Summarization model not available.\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if hasattr(model, 'generate'):  # OpenVINO model\n",
    "            result = model(text, max_length=max_length, min_length=30, do_sample=False)\n",
    "        else:  # HuggingFace pipeline\n",
    "            result = model(text, max_length=max_length, min_length=30, do_sample=False)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            summary = result[0].get('summary_text', str(result[0]))\n",
    "        else:\n",
    "            summary = str(result)\n",
    "        \n",
    "        response = f\"**Summary:** {summary}\\n\\n\"\n",
    "        response += f\"**Original Length:** {len(text)} characters\\n\"\n",
    "        response += f\"**Summary Length:** {len(summary)} characters\\n\"\n",
    "        response += f\"**Compression Ratio:** {len(summary)/len(text):.2%}\\n\"\n",
    "        response += f\"**Processing Time:** {processing_time:.3f} seconds\"\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error summarizing text: {str(e)}\"\n",
    "\n",
    "def analyze_learning_content(content: str) -> str:\n",
    "    \"\"\"Analyze educational content and provide insights\"\"\"\n",
    "    if not content.strip():\n",
    "        return \"Please provide content to analyze.\"\n",
    "    \n",
    "    # Basic educational content analysis\n",
    "    words = content.split()\n",
    "    sentences = content.split('.')\n",
    "    \n",
    "    # Calculate readability metrics\n",
    "    avg_words_per_sentence = len(words) / len(sentences) if sentences else 0\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    \n",
    "    # Determine difficulty level\n",
    "    if avg_word_length < 5 and avg_words_per_sentence < 15:\n",
    "        difficulty = \"Beginner\"\n",
    "    elif avg_word_length < 6 and avg_words_per_sentence < 20:\n",
    "        difficulty = \"Intermediate\"\n",
    "    else:\n",
    "        difficulty = \"Advanced\"\n",
    "    \n",
    "    # Estimate reading time (200 WPM average)\n",
    "    reading_time = len(words) / 200\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarize_text(content)\n",
    "    \n",
    "    analysis = f\"\"\"\n",
    "**üìä Content Analysis:**\n",
    "\n",
    "**üìà Metrics:**\n",
    "- Word Count: {len(words)}\n",
    "- Sentence Count: {len(sentences)}\n",
    "- Average Words per Sentence: {avg_words_per_sentence:.1f}\n",
    "- Average Word Length: {avg_word_length:.1f} characters\n",
    "- Estimated Reading Time: {reading_time:.1f} minutes\n",
    "\n",
    "**üéØ Difficulty Level:** {difficulty}\n",
    "\n",
    "**üìù Summary:**\n",
    "{summary}\n",
    "\n",
    "**üí° Teaching Suggestions:**\n",
    "- Break down into {max(1, len(words)//100)} smaller sections\n",
    "- Add visual aids for complex concepts\n",
    "- Include interactive elements every {max(1, int(reading_time))} minutes\n",
    "    \"\"\"\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Create Gradio interface\n",
    "def create_learning_assistant_interface():\n",
    "    \"\"\"Create the main Gradio interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"üéì AI Learning Assistant\", theme=gr.themes.Soft()) as interface:\n",
    "        \n",
    "        gr.Markdown(\"\"\"\n",
    "        # üéì AI-Powered Interactive Learning Assistant\n",
    "        \n",
    "        Welcome to your intelligent classroom companion! This assistant helps with:\n",
    "        - ‚ùì **Question Answering**: Get instant answers to your questions\n",
    "        - üìù **Content Summarization**: Summarize lessons and materials  \n",
    "        - üîç **Learning Analysis**: Analyze content difficulty and provide teaching tips\n",
    "        \n",
    "        *Powered by OpenVINO for optimal performance*\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tab(\"‚ùì Question & Answer\"):\n",
    "            gr.Markdown(\"### Ask any educational question and get instant answers!\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    question_input = gr.Textbox(\n",
    "                        label=\"Your Question\",\n",
    "                        placeholder=\"What is photosynthesis?\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                    context_input = gr.Textbox(\n",
    "                        label=\"Context (optional)\",\n",
    "                        placeholder=\"Provide additional context or lesson material...\",\n",
    "                        lines=4\n",
    "                    )\n",
    "                    ask_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    answer_output = gr.Markdown(label=\"Answer\")\n",
    "            \n",
    "            ask_btn.click(\n",
    "                fn=process_question,\n",
    "                inputs=[question_input, context_input],\n",
    "                outputs=answer_output\n",
    "            )\n",
    "            \n",
    "            # Example questions\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [\"What is machine learning?\", \"Machine learning is a subset of artificial intelligence.\"],\n",
    "                    [\"How does photosynthesis work?\", \"Photosynthesis is the process by which plants make food using sunlight.\"],\n",
    "                    [\"What caused World War II?\", \"World War II was a global conflict from 1939-1945.\"]\n",
    "                ],\n",
    "                inputs=[question_input, context_input]\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"üìù Text Summarization\"):\n",
    "            gr.Markdown(\"### Summarize any educational content quickly!\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Text to Summarize\",\n",
    "                        placeholder=\"Paste your lesson content, article, or any text here...\",\n",
    "                        lines=10\n",
    "                    )\n",
    "                    max_length_slider = gr.Slider(\n",
    "                        minimum=50,\n",
    "                        maximum=300,\n",
    "                        value=150,\n",
    "                        step=10,\n",
    "                        label=\"Summary Length (words)\"\n",
    "                    )\n",
    "                    summarize_btn = gr.Button(\"Generate Summary\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    summary_output = gr.Markdown(label=\"Summary\")\n",
    "            \n",
    "            summarize_btn.click(\n",
    "                fn=summarize_text,\n",
    "                inputs=[text_input, max_length_slider],\n",
    "                outputs=summary_output\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"üîç Learning Analysis\"):\n",
    "            gr.Markdown(\"### Analyze educational content and get teaching insights!\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    content_input = gr.Textbox(\n",
    "                        label=\"Educational Content\",\n",
    "                        placeholder=\"Paste lesson content for analysis...\",\n",
    "                        lines=12\n",
    "                    )\n",
    "                    analyze_btn = gr.Button(\"Analyze Content\", variant=\"primary\")\n",
    "                \n",
    "                with gr.Column():\n",
    "                    analysis_output = gr.Markdown(label=\"Analysis Results\")\n",
    "            \n",
    "            analyze_btn.click(\n",
    "                fn=analyze_learning_content,\n",
    "                inputs=content_input,\n",
    "                outputs=analysis_output\n",
    "            )\n",
    "        \n",
    "        with gr.Tab(\"üìä Performance Monitor\"):\n",
    "            gr.Markdown(\"### System Performance and Model Information\")\n",
    "            \n",
    "            # Display model status\n",
    "            model_status = \"\"\n",
    "            for task, model in ai_models.items():\n",
    "                status = \"‚úÖ Loaded\" if model else \"‚ùå Not Available\"\n",
    "                model_status += f\"- **{task.title()}**: {status}\\n\"\n",
    "            \n",
    "            if openvino_models:\n",
    "                model_status += \"\\n**OpenVINO Models:**\\n\"\n",
    "                for task, model in openvino_models.items():\n",
    "                    status = \"‚úÖ Optimized\" if model else \"‚ùå Not Available\" \n",
    "                    model_status += f\"- **{task.title()}**: {status}\\n\"\n",
    "            \n",
    "            gr.Markdown(f\"### Model Status\\n{model_status}\")\n",
    "            \n",
    "            # Display performance data if available\n",
    "            if benchmark_results:\n",
    "                perf_text = \"### Performance Benchmarks\\n\\n\"\n",
    "                for model_name, results in benchmark_results.items():\n",
    "                    if \"error\" not in results:\n",
    "                        perf_text += f\"**{results['model_name']}:**\\n\"\n",
    "                        perf_text += f\"- Mean Time: {results['mean_time']:.3f}s\\n\"\n",
    "                        perf_text += f\"- Best Time: {results['min_time']:.3f}s\\n\\n\"\n",
    "                \n",
    "                gr.Markdown(perf_text)\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Launch the interface\n",
    "if gradio_available:\n",
    "    print(\"\\nüöÄ Creating Gradio interface...\")\n",
    "    demo = create_learning_assistant_interface()\n",
    "    \n",
    "    print(\"‚úÖ Interface created successfully!\")\n",
    "    print(\"üí° Run demo.launch() to start the interactive interface\")\n",
    "    \n",
    "    # To launch immediately (uncomment the line below):\n",
    "    # demo.launch(share=True, debug=True)\n",
    "else:\n",
    "    print(\"‚ùå Could not create interface - Gradio not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfb6a8",
   "metadata": {},
   "source": [
    "## 7. Integrate Backend Logic for Multimodal Interaction\n",
    "\n",
    "### üîÄ Input Routing Strategy\n",
    "\n",
    "Our system intelligently routes different input types to appropriate AI models:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Input] --> B{Input Type?}\n",
    "    B -->|Text Question| C[Question Answering Model]\n",
    "    B -->|Long Text| D[Summarization Model]\n",
    "    B -->|Audio File| E[Speech Recognition Model]\n",
    "    B -->|Image| F[Image Captioning Model]\n",
    "    C --> G[Educational Enhancement]\n",
    "    D --> G\n",
    "    E --> G\n",
    "    F --> G\n",
    "    G --> H[Response Generation]\n",
    "    H --> I[Output Formatting]\n",
    "```\n",
    "\n",
    "### üéØ Multimodal Processing Pipeline\n",
    "\n",
    "1. **Input Detection**: Identify content type and format\n",
    "2. **Preprocessing**: Clean and prepare data for models\n",
    "3. **Model Selection**: Route to appropriate AI model\n",
    "4. **Processing**: Generate initial response\n",
    "5. **Enhancement**: Add educational features\n",
    "6. **Output**: Format for user consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multimodal processing pipeline\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Any, Optional\n",
    "\n",
    "class InputType(Enum):\n",
    "    TEXT_QUESTION = \"text_question\"\n",
    "    TEXT_DOCUMENT = \"text_document\"\n",
    "    AUDIO_FILE = \"audio_file\"\n",
    "    IMAGE_FILE = \"image_file\"\n",
    "    MULTIMODAL = \"multimodal\"\n",
    "\n",
    "@dataclass\n",
    "class ProcessingResult:\n",
    "    \"\"\"Structured result from processing pipeline\"\"\"\n",
    "    input_type: InputType\n",
    "    original_input: Any\n",
    "    processed_output: str\n",
    "    confidence: float\n",
    "    processing_time: float\n",
    "    educational_enhancements: dict\n",
    "    metadata: dict\n",
    "\n",
    "class MultimodalProcessor:\n",
    "    \"\"\"Main processor for handling different input types\"\"\"\n",
    "    \n",
    "    def __init__(self, models_dict: dict, openvino_models_dict: dict = None):\n",
    "        self.models = models_dict\n",
    "        self.openvino_models = openvino_models_dict or {}\n",
    "        \n",
    "    def detect_input_type(self, input_data: Any) -> InputType:\n",
    "        \"\"\"Detect the type of input provided\"\"\"\n",
    "        \n",
    "        if isinstance(input_data, str):\n",
    "            # Analyze text to determine if it's a question or document\n",
    "            text = input_data.strip().lower()\n",
    "            \n",
    "            # Check for question indicators\n",
    "            question_words = [\"what\", \"why\", \"how\", \"when\", \"where\", \"who\", \"can\", \"could\", \"would\", \"should\"]\n",
    "            is_question = (\n",
    "                text.endswith(\"?\") or\n",
    "                any(text.startswith(word) for word in question_words) or\n",
    "                len(text.split()) < 20  # Short text likely a question\n",
    "            )\n",
    "            \n",
    "            return InputType.TEXT_QUESTION if is_question else InputType.TEXT_DOCUMENT\n",
    "            \n",
    "        elif hasattr(input_data, 'name') and input_data.name:\n",
    "            # File-like object\n",
    "            filename = input_data.name.lower()\n",
    "            if filename.endswith(('.mp3', '.wav', '.m4a', '.ogg')):\n",
    "                return InputType.AUDIO_FILE\n",
    "            elif filename.endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "                return InputType.IMAGE_FILE\n",
    "                \n",
    "        return InputType.TEXT_QUESTION  # Default fallback\n",
    "    \n",
    "    def get_best_model(self, task: str):\n",
    "        \"\"\"Get the best available model for a task (OpenVINO preferred)\"\"\"\n",
    "        # Try OpenVINO first, then fall back to original\n",
    "        model_mapping = {\n",
    "            \"question_answering\": [\"qa\", \"question_answering\"],\n",
    "            \"summarization\": [\"summarization\", \"summarization\"],\n",
    "            \"text_generation\": [\"text_generation\", \"text_generation\"]\n",
    "        }\n",
    "        \n",
    "        possible_keys = model_mapping.get(task, [task])\n",
    "        \n",
    "        # Check OpenVINO models first\n",
    "        for key in possible_keys:\n",
    "            if key in self.openvino_models and self.openvino_models[key] is not None:\n",
    "                return self.openvino_models[key], \"openvino\"\n",
    "        \n",
    "        # Fall back to original models\n",
    "        for key in possible_keys:\n",
    "            if key in self.models and self.models[key] is not None:\n",
    "                return self.models[key], \"original\"\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def process_text_question(self, question: str, context: str = \"\") -> ProcessingResult:\n",
    "        \"\"\"Process a text question\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model, model_type = self.get_best_model(\"question_answering\")\n",
    "        \n",
    "        if model is None:\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_QUESTION,\n",
    "                original_input=question,\n",
    "                processed_output=\"‚ùå Question answering model not available\",\n",
    "                confidence=0.0,\n",
    "                processing_time=0.0,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"error\": \"Model not available\"}\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Provide educational context if none given\n",
    "            if not context.strip():\n",
    "                context = \"This is an educational question in a classroom setting. Please provide a clear, educational answer suitable for learning.\"\n",
    "            \n",
    "            # Process with model\n",
    "            result = model(question=question, context=context)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Extract answer and confidence\n",
    "            if isinstance(result, dict):\n",
    "                answer = result.get('answer', str(result))\n",
    "                confidence = result.get('score', 0.0)\n",
    "            else:\n",
    "                answer = str(result)\n",
    "                confidence = 0.8  # Default confidence\n",
    "            \n",
    "            # Generate educational enhancements\n",
    "            enhancements = self.generate_educational_enhancements(question, answer)\n",
    "            \n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_QUESTION,\n",
    "                original_input=question,\n",
    "                processed_output=answer,\n",
    "                confidence=confidence,\n",
    "                processing_time=processing_time,\n",
    "                educational_enhancements=enhancements,\n",
    "                metadata={\"model_type\": model_type, \"context_provided\": bool(context.strip())}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_QUESTION,\n",
    "                original_input=question,\n",
    "                processed_output=f\"‚ùå Error processing question: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                processing_time=time.time() - start_time,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def process_text_document(self, text: str, max_length: int = 150) -> ProcessingResult:\n",
    "        \"\"\"Process a text document for summarization\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model, model_type = self.get_best_model(\"summarization\")\n",
    "        \n",
    "        if model is None:\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_DOCUMENT,\n",
    "                original_input=text,\n",
    "                processed_output=\"‚ùå Summarization model not available\",\n",
    "                confidence=0.0,\n",
    "                processing_time=0.0,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"error\": \"Model not available\"}\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Process with model\n",
    "            result = model(text, max_length=max_length, min_length=30, do_sample=False)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Extract summary\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                summary = result[0].get('summary_text', str(result[0]))\n",
    "            else:\n",
    "                summary = str(result)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            compression_ratio = len(summary) / len(text) if len(text) > 0 else 0\n",
    "            confidence = min(1.0, max(0.0, 1.0 - compression_ratio))  # Higher compression = lower confidence\n",
    "            \n",
    "            # Generate educational enhancements\n",
    "            enhancements = {\n",
    "                \"original_length\": len(text),\n",
    "                \"summary_length\": len(summary),\n",
    "                \"compression_ratio\": compression_ratio,\n",
    "                \"reading_time_original\": len(text.split()) / 200,  # 200 WPM\n",
    "                \"reading_time_summary\": len(summary.split()) / 200,\n",
    "                \"key_topics\": self.extract_key_topics(text),\n",
    "                \"difficulty_level\": self.assess_difficulty(text)\n",
    "            }\n",
    "            \n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_DOCUMENT,\n",
    "                original_input=text,\n",
    "                processed_output=summary,\n",
    "                confidence=confidence,\n",
    "                processing_time=processing_time,\n",
    "                educational_enhancements=enhancements,\n",
    "                metadata={\"model_type\": model_type}\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.TEXT_DOCUMENT,\n",
    "                original_input=text,\n",
    "                processed_output=f\"‚ùå Error summarizing text: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                processing_time=time.time() - start_time,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"error\": str(e)}\n",
    "            )\n",
    "    \n",
    "    def generate_educational_enhancements(self, question: str, answer: str) -> dict:\n",
    "        \"\"\"Generate educational enhancements for Q&A\"\"\"\n",
    "        enhancements = {}\n",
    "        \n",
    "        # Generate follow-up questions\n",
    "        question_lower = question.lower()\n",
    "        if \"what\" in question_lower:\n",
    "            enhancements[\"follow_up_questions\"] = [\n",
    "                \"Why is this important?\",\n",
    "                \"How does this relate to other concepts?\",\n",
    "                \"Can you provide an example?\"\n",
    "            ]\n",
    "        elif \"why\" in question_lower:\n",
    "            enhancements[\"follow_up_questions\"] = [\n",
    "                \"What are the implications?\",\n",
    "                \"How could this be different?\",\n",
    "                \"What evidence supports this?\"\n",
    "            ]\n",
    "        elif \"how\" in question_lower:\n",
    "            enhancements[\"follow_up_questions\"] = [\n",
    "                \"What are the key steps?\",\n",
    "                \"Why does this method work?\",\n",
    "                \"Are there alternative approaches?\"\n",
    "            ]\n",
    "        else:\n",
    "            enhancements[\"follow_up_questions\"] = [\n",
    "                \"What would you like to know more about?\",\n",
    "                \"How can this be applied?\",\n",
    "                \"What are related concepts?\"\n",
    "            ]\n",
    "        \n",
    "        # Generate learning tips\n",
    "        enhancements[\"learning_tips\"] = [\n",
    "            \"Try to connect this to what you already know\",\n",
    "            \"Practice explaining this concept in your own words\",\n",
    "            \"Look for real-world examples of this concept\"\n",
    "        ]\n",
    "        \n",
    "        # Assess complexity\n",
    "        complexity_score = (len(answer.split()) / 50) + (len([w for w in answer.split() if len(w) > 6]) / 10)\n",
    "        if complexity_score < 1:\n",
    "            complexity = \"Simple\"\n",
    "        elif complexity_score < 2:\n",
    "            complexity = \"Moderate\"\n",
    "        else:\n",
    "            complexity = \"Complex\"\n",
    "        \n",
    "        enhancements[\"complexity\"] = complexity\n",
    "        enhancements[\"estimated_read_time\"] = len(answer.split()) / 200  # minutes\n",
    "        \n",
    "        return enhancements\n",
    "    \n",
    "    def extract_key_topics(self, text: str) -> list:\n",
    "        \"\"\"Extract key topics from text (simplified approach)\"\"\"\n",
    "        # Simple keyword extraction - in production, use more sophisticated NLP\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Common educational topic indicators\n",
    "        topic_indicators = [\n",
    "            \"mathematics\", \"science\", \"history\", \"literature\", \"biology\", \n",
    "            \"chemistry\", \"physics\", \"geography\", \"economics\", \"psychology\"\n",
    "        ]\n",
    "        \n",
    "        found_topics = [topic for topic in topic_indicators if topic in text.lower()]\n",
    "        \n",
    "        # Also extract frequently occurring longer words\n",
    "        long_words = [word for word in words if len(word) > 6]\n",
    "        word_freq = {}\n",
    "        for word in long_words:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "        \n",
    "        # Get top frequent words\n",
    "        frequent_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        found_topics.extend([word for word, freq in frequent_words if freq > 1])\n",
    "        \n",
    "        return found_topics[:10]  # Limit to top 10\n",
    "    \n",
    "    def assess_difficulty(self, text: str) -> str:\n",
    "        \"\"\"Assess text difficulty level\"\"\"\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "        \n",
    "        if avg_word_length < 5 and avg_sentence_length < 15:\n",
    "            return \"Beginner\"\n",
    "        elif avg_word_length < 6 and avg_sentence_length < 20:\n",
    "            return \"Intermediate\"\n",
    "        else:\n",
    "            return \"Advanced\"\n",
    "    \n",
    "    def process(self, input_data: Any, **kwargs) -> ProcessingResult:\n",
    "        \"\"\"Main processing method that routes to appropriate handler\"\"\"\n",
    "        \n",
    "        input_type = self.detect_input_type(input_data)\n",
    "        \n",
    "        if input_type == InputType.TEXT_QUESTION:\n",
    "            context = kwargs.get('context', '')\n",
    "            return self.process_text_question(input_data, context)\n",
    "            \n",
    "        elif input_type == InputType.TEXT_DOCUMENT:\n",
    "            max_length = kwargs.get('max_length', 150)\n",
    "            return self.process_text_document(input_data, max_length)\n",
    "            \n",
    "        elif input_type == InputType.AUDIO_FILE:\n",
    "            # Placeholder for audio processing\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.AUDIO_FILE,\n",
    "                original_input=input_data,\n",
    "                processed_output=\"üé§ Audio processing not implemented in this demo\",\n",
    "                confidence=0.0,\n",
    "                processing_time=0.0,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"note\": \"Would use speech recognition model\"}\n",
    "            )\n",
    "            \n",
    "        elif input_type == InputType.IMAGE_FILE:\n",
    "            # Placeholder for image processing\n",
    "            return ProcessingResult(\n",
    "                input_type=InputType.IMAGE_FILE,\n",
    "                original_input=input_data,\n",
    "                processed_output=\"üñºÔ∏è Image processing not implemented in this demo\",\n",
    "                confidence=0.0,\n",
    "                processing_time=0.0,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"note\": \"Would use image captioning model\"}\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            return ProcessingResult(\n",
    "                input_type=input_type,\n",
    "                original_input=input_data,\n",
    "                processed_output=\"‚ùå Unsupported input type\",\n",
    "                confidence=0.0,\n",
    "                processing_time=0.0,\n",
    "                educational_enhancements={},\n",
    "                metadata={\"error\": \"Unsupported input\"}\n",
    "            )\n",
    "\n",
    "# Initialize the multimodal processor\n",
    "processor = MultimodalProcessor(ai_models, openvino_models)\n",
    "\n",
    "print(\"‚úÖ Multimodal processor initialized!\")\n",
    "print(f\"üìä Available models: {list(ai_models.keys())}\")\n",
    "if openvino_models:\n",
    "    print(f\"üöÄ OpenVINO models: {list(openvino_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08186381",
   "metadata": {},
   "source": [
    "## 8. Implement Personalization and Progress Tracking\n",
    "\n",
    "### üéØ Personalization Strategy\n",
    "\n",
    "Our learning assistant adapts to individual students through:\n",
    "\n",
    "1. **Learning Style Detection**: Analyze preferred interaction modes\n",
    "2. **Difficulty Adaptation**: Adjust response complexity based on comprehension\n",
    "3. **Topic Tracking**: Monitor subject areas and knowledge gaps\n",
    "4. **Performance Analytics**: Track response times and accuracy trends\n",
    "\n",
    "### üìä Progress Tracking Features\n",
    "\n",
    "- **Session History**: Store questions, answers, and interactions\n",
    "- **Learning Patterns**: Identify strengths and improvement areas\n",
    "- **Engagement Metrics**: Monitor interaction frequency and depth\n",
    "- **Adaptive Responses**: Customize explanations based on student level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c885957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement personalization and progress tracking\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class StudentProfile:\n",
    "    \"\"\"Student learning profile and preferences\"\"\"\n",
    "    student_id: str\n",
    "    name: str = \"\"\n",
    "    grade_level: str = \"\"\n",
    "    learning_style: str = \"visual\"  # visual, auditory, kinesthetic\n",
    "    preferred_complexity: str = \"intermediate\"  # beginner, intermediate, advanced\n",
    "    strong_subjects: list = None\n",
    "    weak_subjects: list = None\n",
    "    interaction_history: list = None\n",
    "    performance_metrics: dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.strong_subjects is None:\n",
    "            self.strong_subjects = []\n",
    "        if self.weak_subjects is None:\n",
    "            self.weak_subjects = []\n",
    "        if self.interaction_history is None:\n",
    "            self.interaction_history = []\n",
    "        if self.performance_metrics is None:\n",
    "            self.performance_metrics = {\n",
    "                \"total_questions\": 0,\n",
    "                \"correct_answers\": 0,\n",
    "                \"avg_response_time\": 0.0,\n",
    "                \"engagement_score\": 0.0,\n",
    "                \"last_activity\": None\n",
    "            }\n",
    "\n",
    "class LearningAnalytics:\n",
    "    \"\"\"Analyze learning patterns and provide insights\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.students = {}  # student_id -> StudentProfile\n",
    "        self.session_data = defaultdict(list)  # session_id -> interactions\n",
    "    \n",
    "    def get_or_create_student(self, student_id: str, **kwargs) -> StudentProfile:\n",
    "        \"\"\"Get existing student profile or create new one\"\"\"\n",
    "        if student_id not in self.students:\n",
    "            self.students[student_id] = StudentProfile(student_id=student_id, **kwargs)\n",
    "        return self.students[student_id]\n",
    "    \n",
    "    def record_interaction(self, student_id: str, interaction_data: dict):\n",
    "        \"\"\"Record a learning interaction\"\"\"\n",
    "        student = self.get_or_create_student(student_id)\n",
    "        \n",
    "        # Add timestamp\n",
    "        interaction_data[\"timestamp\"] = datetime.now().isoformat()\n",
    "        \n",
    "        # Store in student history\n",
    "        student.interaction_history.append(interaction_data)\n",
    "        \n",
    "        # Update performance metrics\n",
    "        self.update_performance_metrics(student, interaction_data)\n",
    "        \n",
    "        # Limit history size\n",
    "        if len(student.interaction_history) > 100:\n",
    "            student.interaction_history = student.interaction_history[-100:]\n",
    "    \n",
    "    def update_performance_metrics(self, student: StudentProfile, interaction: dict):\n",
    "        \"\"\"Update student performance metrics\"\"\"\n",
    "        metrics = student.performance_metrics\n",
    "        \n",
    "        # Update total questions\n",
    "        if interaction.get(\"type\") == \"question\":\n",
    "            metrics[\"total_questions\"] += 1\n",
    "            \n",
    "            # Update response time\n",
    "            response_time = interaction.get(\"processing_time\", 0)\n",
    "            if metrics[\"avg_response_time\"] == 0:\n",
    "                metrics[\"avg_response_time\"] = response_time\n",
    "            else:\n",
    "                # Running average\n",
    "                total_questions = metrics[\"total_questions\"]\n",
    "                metrics[\"avg_response_time\"] = (\n",
    "                    (metrics[\"avg_response_time\"] * (total_questions - 1) + response_time) / total_questions\n",
    "                )\n",
    "            \n",
    "            # Update engagement score based on interaction quality\n",
    "            confidence = interaction.get(\"confidence\", 0.5)\n",
    "            metrics[\"engagement_score\"] = (\n",
    "                metrics[\"engagement_score\"] * 0.9 + confidence * 0.1\n",
    "            )\n",
    "        \n",
    "        # Update last activity\n",
    "        metrics[\"last_activity\"] = datetime.now().isoformat()\n",
    "    \n",
    "    def analyze_learning_style(self, student_id: str) -> dict:\n",
    "        \"\"\"Analyze student's learning style preferences\"\"\"\n",
    "        student = self.students.get(student_id)\n",
    "        if not student or not student.interaction_history:\n",
    "            return {\"style\": \"visual\", \"confidence\": 0.0}\n",
    "        \n",
    "        # Analyze interaction patterns\n",
    "        interaction_types = Counter()\n",
    "        success_by_type = defaultdict(list)\n",
    "        \n",
    "        for interaction in student.interaction_history[-20:]:  # Last 20 interactions\n",
    "            interaction_type = interaction.get(\"input_type\", \"text\")\n",
    "            confidence = interaction.get(\"confidence\", 0.5)\n",
    "            \n",
    "            interaction_types[interaction_type] += 1\n",
    "            success_by_type[interaction_type].append(confidence)\n",
    "        \n",
    "        # Determine preferred style\n",
    "        style_mapping = {\n",
    "            \"text_question\": \"visual\",\n",
    "            \"text_document\": \"visual\", \n",
    "            \"audio_file\": \"auditory\",\n",
    "            \"image_file\": \"visual\"\n",
    "        }\n",
    "        \n",
    "        style_scores = defaultdict(float)\n",
    "        for itype, count in interaction_types.items():\n",
    "            style = style_mapping.get(itype, \"visual\")\n",
    "            avg_success = np.mean(success_by_type[itype]) if success_by_type[itype] else 0.5\n",
    "            style_scores[style] += count * avg_success\n",
    "        \n",
    "        if style_scores:\n",
    "            best_style = max(style_scores.items(), key=lambda x: x[1])\n",
    "            total_score = sum(style_scores.values())\n",
    "            confidence = best_style[1] / total_score if total_score > 0 else 0.0\n",
    "            \n",
    "            return {\"style\": best_style[0], \"confidence\": confidence}\n",
    "        \n",
    "        return {\"style\": \"visual\", \"confidence\": 0.0}\n",
    "    \n",
    "    def assess_knowledge_level(self, student_id: str, subject: str = None) -> dict:\n",
    "        \"\"\"Assess student's knowledge level in a subject\"\"\"\n",
    "        student = self.students.get(student_id)\n",
    "        if not student or not student.interaction_history:\n",
    "            return {\"level\": \"beginner\", \"confidence\": 0.0}\n",
    "        \n",
    "        # Filter interactions by subject if specified\n",
    "        relevant_interactions = student.interaction_history\n",
    "        if subject:\n",
    "            relevant_interactions = [\n",
    "                i for i in student.interaction_history \n",
    "                if subject.lower() in i.get(\"original_input\", \"\").lower()\n",
    "            ]\n",
    "        \n",
    "        if not relevant_interactions:\n",
    "            return {\"level\": \"beginner\", \"confidence\": 0.0}\n",
    "        \n",
    "        # Analyze complexity of questions asked and confidence of answers\n",
    "        complexities = []\n",
    "        confidences = []\n",
    "        \n",
    "        for interaction in relevant_interactions[-10:]:  # Last 10 relevant interactions\n",
    "            # Get complexity from educational enhancements\n",
    "            enhancements = interaction.get(\"educational_enhancements\", {})\n",
    "            complexity = enhancements.get(\"complexity\", \"Simple\")\n",
    "            \n",
    "            complexity_score = {\"Simple\": 1, \"Moderate\": 2, \"Complex\": 3}.get(complexity, 1)\n",
    "            complexities.append(complexity_score)\n",
    "            \n",
    "            confidence = interaction.get(\"confidence\", 0.5)\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        if complexities and confidences:\n",
    "            avg_complexity = np.mean(complexities)\n",
    "            avg_confidence = np.mean(confidences)\n",
    "            \n",
    "            # Determine level based on complexity handled and confidence\n",
    "            level_score = avg_complexity * avg_confidence\n",
    "            \n",
    "            if level_score < 1.5:\n",
    "                level = \"beginner\"\n",
    "            elif level_score < 2.5:\n",
    "                level = \"intermediate\"  \n",
    "            else:\n",
    "                level = \"advanced\"\n",
    "            \n",
    "            return {\"level\": level, \"confidence\": avg_confidence}\n",
    "        \n",
    "        return {\"level\": \"beginner\", \"confidence\": 0.0}\n",
    "    \n",
    "    def generate_personalized_response(self, student_id: str, base_response: str, \n",
    "                                     interaction_type: str) -> str:\n",
    "        \"\"\"Generate personalized response based on student profile\"\"\"\n",
    "        student = self.students.get(student_id)\n",
    "        if not student:\n",
    "            return base_response\n",
    "        \n",
    "        # Analyze student's learning style and level\n",
    "        style_analysis = self.analyze_learning_style(student_id)\n",
    "        learning_style = style_analysis[\"style\"]\n",
    "        \n",
    "        # Get performance metrics\n",
    "        metrics = student.performance_metrics\n",
    "        engagement = metrics.get(\"engagement_score\", 0.5)\n",
    "        \n",
    "        # Customize response based on learning style\n",
    "        if learning_style == \"visual\":\n",
    "            suggestion = \"\\n\\nüí° **Visual Learning Tip**: Try creating a diagram or mind map of this concept.\"\n",
    "        elif learning_style == \"auditory\":\n",
    "            suggestion = \"\\n\\nüéµ **Auditory Learning Tip**: Try explaining this concept out loud or discussing it with others.\"\n",
    "        else:  # kinesthetic\n",
    "            suggestion = \"\\n\\nü§≤ **Hands-on Learning Tip**: Look for ways to practice or apply this concept.\"\n",
    "        \n",
    "        # Add encouragement based on engagement\n",
    "        if engagement < 0.4:\n",
    "            encouragement = \"\\n\\n‚ú® **Keep going!** Learning takes time, and you're making progress.\"\n",
    "        elif engagement > 0.8:\n",
    "            encouragement = \"\\n\\nüåü **Great job!** You're showing excellent understanding.\"\n",
    "        else:\n",
    "            encouragement = \"\\n\\nüëç **Nice work!** You're building good knowledge.\"\n",
    "        \n",
    "        return base_response + suggestion + encouragement\n",
    "    \n",
    "    def get_learning_dashboard(self, student_id: str) -> dict:\n",
    "        \"\"\"Generate learning dashboard for student\"\"\"\n",
    "        student = self.students.get(student_id)\n",
    "        if not student:\n",
    "            return {\"error\": \"Student not found\"}\n",
    "        \n",
    "        # Recent activity\n",
    "        recent_interactions = student.interaction_history[-10:] if student.interaction_history else []\n",
    "        \n",
    "        # Performance trends\n",
    "        metrics = student.performance_metrics\n",
    "        \n",
    "        # Learning style analysis\n",
    "        style_analysis = self.analyze_learning_style(student_id)\n",
    "        \n",
    "        # Subject analysis\n",
    "        subject_performance = {}\n",
    "        subjects = [\"mathematics\", \"science\", \"history\", \"english\", \"geography\"]\n",
    "        for subject in subjects:\n",
    "            assessment = self.assess_knowledge_level(student_id, subject)\n",
    "            subject_performance[subject] = assessment\n",
    "        \n",
    "        return {\n",
    "            \"student_profile\": {\n",
    "                \"name\": student.name or f\"Student {student_id}\",\n",
    "                \"grade_level\": student.grade_level,\n",
    "                \"learning_style\": style_analysis,\n",
    "                \"total_interactions\": len(student.interaction_history)\n",
    "            },\n",
    "            \"performance_metrics\": metrics,\n",
    "            \"subject_performance\": subject_performance,\n",
    "            \"recent_activity\": recent_interactions[-5:],  # Last 5 interactions\n",
    "            \"recommendations\": self.generate_recommendations(student_id)\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self, student_id: str) -> list:\n",
    "        \"\"\"Generate personalized learning recommendations\"\"\"\n",
    "        student = self.students.get(student_id)\n",
    "        if not student or not student.interaction_history:\n",
    "            return [\"Start by asking questions about topics you're curious about!\"]\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Analyze recent activity\n",
    "        recent = student.interaction_history[-5:] if student.interaction_history else []\n",
    "        if not recent:\n",
    "            return [\"Continue exploring different topics to build your knowledge.\"]\n",
    "        \n",
    "        # Check for engagement patterns\n",
    "        recent_confidences = [i.get(\"confidence\", 0.5) for i in recent]\n",
    "        avg_confidence = np.mean(recent_confidences)\n",
    "        \n",
    "        if avg_confidence < 0.4:\n",
    "            recommendations.append(\"üí™ Try asking simpler questions to build confidence\")\n",
    "            recommendations.append(\"üìö Review basic concepts before tackling complex topics\")\n",
    "        elif avg_confidence > 0.8:\n",
    "            recommendations.append(\"üöÄ Challenge yourself with more advanced questions\")\n",
    "            recommendations.append(\"üîó Explore connections between different topics\")\n",
    "        \n",
    "        # Check interaction diversity\n",
    "        interaction_types = [i.get(\"input_type\", \"text\") for i in recent]\n",
    "        if len(set(interaction_types)) <= 1:\n",
    "            recommendations.append(\"üéØ Try different ways of learning (text, audio, visual)\")\n",
    "        \n",
    "        # Subject-specific recommendations\n",
    "        subjects_asked = []\n",
    "        for interaction in recent:\n",
    "            text = interaction.get(\"original_input\", \"\").lower()\n",
    "            for subject in [\"math\", \"science\", \"history\", \"english\"]:\n",
    "                if subject in text:\n",
    "                    subjects_asked.append(subject)\n",
    "        \n",
    "        if not subjects_asked:\n",
    "            recommendations.append(\"üåç Explore different subjects to find your interests\")\n",
    "        elif len(set(subjects_asked)) <= 1:\n",
    "            recommendations.append(\"üìñ Try asking questions about different subjects\")\n",
    "        \n",
    "        return recommendations[:3]  # Limit to 3 recommendations\n",
    "\n",
    "# Initialize learning analytics\n",
    "analytics = LearningAnalytics()\n",
    "\n",
    "# Demonstrate personalization\n",
    "def demo_personalization():\n",
    "    \"\"\"Demonstrate personalization features\"\"\"\n",
    "    print(\"üéØ Personalization Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create sample student\n",
    "    student_id = \"demo_student_001\"\n",
    "    \n",
    "    # Simulate some interactions\n",
    "    sample_interactions = [\n",
    "        {\n",
    "            \"type\": \"question\",\n",
    "            \"input_type\": \"text_question\",\n",
    "            \"original_input\": \"What is photosynthesis?\",\n",
    "            \"processed_output\": \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
    "            \"confidence\": 0.85,\n",
    "            \"processing_time\": 1.2,\n",
    "            \"educational_enhancements\": {\"complexity\": \"Simple\"}\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"question\", \n",
    "            \"input_type\": \"text_question\",\n",
    "            \"original_input\": \"How does quantum mechanics relate to chemistry?\",\n",
    "            \"processed_output\": \"Quantum mechanics explains atomic behavior that determines chemical bonding.\",\n",
    "            \"confidence\": 0.75,\n",
    "            \"processing_time\": 2.1,\n",
    "            \"educational_enhancements\": {\"complexity\": \"Complex\"}\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"summarization\",\n",
    "            \"input_type\": \"text_document\", \n",
    "            \"original_input\": \"Long text about World War II...\",\n",
    "            \"processed_output\": \"Summary of WWII causes and effects.\",\n",
    "            \"confidence\": 0.9,\n",
    "            \"processing_time\": 1.8,\n",
    "            \"educational_enhancements\": {\"complexity\": \"Moderate\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Record interactions\n",
    "    for interaction in sample_interactions:\n",
    "        analytics.record_interaction(student_id, interaction)\n",
    "    \n",
    "    # Generate dashboard\n",
    "    dashboard = analytics.get_learning_dashboard(student_id)\n",
    "    \n",
    "    print(\"üìä Student Dashboard:\")\n",
    "    print(f\"  Learning Style: {dashboard['student_profile']['learning_style']['style']}\")\n",
    "    print(f\"  Total Interactions: {dashboard['student_profile']['total_interactions']}\")\n",
    "    print(f\"  Engagement Score: {dashboard['performance_metrics']['engagement_score']:.2f}\")\n",
    "    \n",
    "    print(\"\\nüéØ Subject Performance:\")\n",
    "    for subject, performance in dashboard['subject_performance'].items():\n",
    "        print(f\"  {subject.title()}: {performance['level']} (confidence: {performance['confidence']:.2f})\")\n",
    "    \n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    for rec in dashboard['recommendations']:\n",
    "        print(f\"  ‚Ä¢ {rec}\")\n",
    "    \n",
    "    # Demo personalized response\n",
    "    base_response = \"The water cycle involves evaporation, condensation, and precipitation.\"\n",
    "    personalized = analytics.generate_personalized_response(\n",
    "        student_id, base_response, \"text_question\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüé® Personalized Response Example:\")\n",
    "    print(f\"Original: {base_response}\")\n",
    "    print(f\"Personalized: {personalized}\")\n",
    "\n",
    "demo_personalization()\n",
    "\n",
    "print(\"\\n‚úÖ Personalization and progress tracking system implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596ddcc",
   "metadata": {},
   "source": [
    "## 9. Demo: Real-Time Interaction and Performance Metrics\n",
    "\n",
    "### üé≠ Live Demonstration\n",
    "\n",
    "This section provides an interactive demonstration of our AI-powered learning assistant with real-time performance monitoring.\n",
    "\n",
    "### üìä Demo Features\n",
    "\n",
    "1. **Question Answering**: Real-time educational Q&A\n",
    "2. **Content Summarization**: Live text summarization  \n",
    "3. **Multimodal Processing**: Handle different input types\n",
    "4. **Performance Monitoring**: Live metrics and benchmarks\n",
    "5. **Personalization**: Adaptive responses based on student profile\n",
    "\n",
    "### üéØ Performance Targets\n",
    "\n",
    "- **Response Time**: < 2 seconds for questions\n",
    "- **Accuracy**: > 90% confidence for educational content\n",
    "- **Throughput**: Handle 10+ concurrent users\n",
    "- **Memory Usage**: < 4GB RAM utilization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
